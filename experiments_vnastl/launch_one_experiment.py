import sys
import random
import dataclasses
from pathlib import Path
import numpy as np
import pandas as pd
import json
import argparse
import pickle
from time import sleep

from tableshift import get_dataset

if __name__ == '__main__':
    import htcondor
    import classad

####################################################
#  START of: details on which experiments to run.  #
####################################################
NOT_DG_TASKS = (
    "acspubcov",
    "physionet", 
    "nhanes_lead",
    "brfss_blood_pressure",
    "sipp",
    "assistments"
)
MODELS = (
    # "ft_transformer",
    # "histgbm",
    # "mlp",
    "saint",
    # "tabtransformer",
    # "resnet",
    # "xgb",
    # "aldro",
    # "dro",
    "node",
    # "group_dro",
    # "label_group_dro",
)
DG_MODELS = (
    # "dann",
    # "irm",
    # "vrex",
    # "mixup",
    "mmd",
)

def IS_TASK_DG(task):
    for dg_task in  NOT_DG_TASKS:
        if task.startswith(dg_task):
            return False
    return True

@dataclasses.dataclass
class ExperimentConfigs:
    name: str
    model: str
    job_memory_gb: int  # = JOB_MEMORY_GB

    n_trials: int # = dic_args["N_TRIALS"]
    job_cpus: int # = dic_args["JOB_CPUS"]
    job_bid: int # = dic_args["JOB_MIN_BID"]
    job_gpus: int = 0 

    # def __post_init__(self):
    #     self.job_bid = max(self.job_bid, dic_args["JOB_MIN_BID"]) 

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--DATA_DIR",
                        help="Directory to load data files from.")
    parser.add_argument("--RESULTS_DIR",
                        help="Directory to save result files to.")
    parser.add_argument("--CLUSTER_LOGS_SAVE_DIR",
                        help="Directory to save cluster logs and job stdout/stderr.")
    parser.add_argument("--N_TRIALS", default="1",
                        help="Number of experiments to run per model, per dataset.")
    parser.add_argument("--JOB_CPUS", default="1",
                        help="Number of CPUs per experiment (per cluster job).")
    parser.add_argument("--JOB_MEMORY_GB", default="64",
                        help="GBs of memory.")
    parser.add_argument("--JOB_MIN_BID", default="15",
                        help="Htcondor bid.")
    parser.add_argument("--task", default="diabetes_readmission",
                        help="Task to run. Overridden when debug=True.")
    args = parser.parse_args()

    dic_args = vars(args)
    
    DATA_DIR_PREPROCESSED = Path(dic_args["DATA_DIR"]) / "preprocessed"
    DATA_DIR_PREPROCESSED.mkdir(exist_ok=True)

    CLUSTER_LOGS_SAVE_ERR_DIR = Path(dic_args["CLUSTER_LOGS_SAVE_DIR"]) / "error" / "experiments"
    CLUSTER_LOGS_SAVE_ERR_DIR.mkdir(exist_ok=True)

    CLUSTER_LOGS_SAVE_OUT_DIR = Path(dic_args["CLUSTER_LOGS_SAVE_DIR"]) / "output" / "experiments"
    CLUSTER_LOGS_SAVE_OUT_DIR.mkdir(exist_ok=True)

    CLUSTER_LOGS_SAVE_LOG_DIR = Path(dic_args["CLUSTER_LOGS_SAVE_DIR"]) / "logs" / "experiments"
    CLUSTER_LOGS_SAVE_LOG_DIR.mkdir(exist_ok=True)

      # enforce min bid

    # Set up experiments for the task
    all_task_experiments = []

    if IS_TASK_DG(dic_args["task"]):
        TASK_MODELS = MODELS + DG_MODELS
    else:
        TASK_MODELS = MODELS

    job_memory_gb = dic_args["JOB_MEMORY_GB"] #BIG_JOB_MEMORY_GB

    for model in TASK_MODELS:
        all_task_experiments.append(
            ExperimentConfigs(
                name=dic_args["task"],
                model=model,
                job_memory_gb=job_memory_gb,
                n_trials = int(dic_args["N_TRIALS"]),
                job_cpus = int(dic_args["JOB_CPUS"]),
                job_bid = int(dic_args["JOB_MIN_BID"]),
                 ))

    # Set up preprocessing of data for the task
    dset = get_dataset(dic_args["task"], dic_args["DATA_DIR"])
    with open(f"{str(DATA_DIR_PREPROCESSED)}/{dic_args['task']}.pickle", 'wb') as f:
        pickle.dump(dset, f)
    # pickle dset into cache_dir under the name of dset_task
    
            
##################################################
#  END of: details on which experiments to run.  #
##################################################

    def launch_experiments_jobs(
        task: str,
        exp_obj: ExperimentConfigs,
    ):
        """Launches the cluster jobs to execute all `n_trials` of a given experiment.

        Parameters
        ----------
        task : str
            The name of the task/data to use.
        exp_obj : ExperimentConfigs
            The detailed configs to run an experiment.
        """

        # Name/prefix for cluster logs related to this job
        cluster_job_err_name = str(
            CLUSTER_LOGS_SAVE_ERR_DIR
            / f"{exp_obj.name}_{exp_obj.model}_$(Cluster).$(Process)"
        )

        cluster_job_out_name = str(
            CLUSTER_LOGS_SAVE_OUT_DIR
            / f"{exp_obj.name}_{exp_obj.model}_$(Cluster).$(Process)"
        )

        cluster_job_log_name = str(
            CLUSTER_LOGS_SAVE_LOG_DIR
            / f"{exp_obj.name}_{exp_obj.model}_$(Cluster).$(Process)"
        )

        EXP_RESULTS_DIR = Path(dic_args["RESULTS_DIR"])
        EXP_RESULTS_DIR.mkdir(exist_ok=True, parents=False)

        # Construct job description
        job_description = htcondor.Submit({
            "executable": "/home/vnastl/miniconda3/envs/tableshift/bin/python3",  # correct env for the python executable
            # "arguments": "foo.py",    # NOTE: used for testing
            "arguments": (
                "/home/vnastl/tableshift/experiments_vnastl/run_experiment_on_cluster.py "
                f"--experiment {exp_obj.name} "
                f"--model {exp_obj.model} "
                f"--cache_dir {str(DATA_DIR_PREPROCESSED)} "
                f"--save_dir {str(EXP_RESULTS_DIR)} "
                f"--trial $(Process) "
                # f"{'--verbose' if VERBOSE else ''} "
            ),
            "output": f"{cluster_job_out_name}.out",
            "error": f"{cluster_job_err_name}.err",
            "log": f"{cluster_job_log_name}.log",
            "request_cpus": f"{exp_obj.job_cpus}",
            "request_gpus": f"{exp_obj.job_gpus}",
            "request_memory": f"{exp_obj.job_memory_gb}GB",
            # "request_disk": "2GB",
            "jobprio": f"{exp_obj.job_bid - 1000}",
            "notify_user": "vivian.nastl@tuebingen.mpg.de",
            "notification": "error",
            # "job_seed_macro": f"$(Process) + {random.randrange(int(1e9))}",      # add random salt to all job seeds
            # "job_seed": "$INT(job_seed_macro)",

            # Concurrency limits:
            # > each job uses this amount of resources out of a pool of 10k
            # "concurrency_limits": "user.theoremfivepointsix:10000",     # 1 job
            # "concurrency_limits": "user.theoremfivepointsix:100",     # 100 jobs in parallel
            "concurrency_limits": "user.theoremfivepointsix:50",     # 200 jobs in parallel

            "+MaxRunningPrice": 100,
            # "+RunningPriceExceededAction": classad.quote("restart"),
            "+MaxTime": 4*3600,
        })

        # Submit `n_trials` jobs to the scheduler
        schedd = htcondor.Schedd()
        submit_result = schedd.submit(job_description, count=exp_obj.n_trials)


    print(
            f"\n*** *** ***\n"
            f"Launching {len(all_task_experiments)} * {dic_args['N_TRIALS']} = "
            f"{dic_args['N_TRIALS'] * len(all_task_experiments)} "
            f"experiments for task={dic_args['task']}"
            f"\n*** *** ***\n"
        )

    for i, exp_obj in enumerate(all_task_experiments):
        print(f"{i}. Launching {exp_obj.n_trials} trials for the experiment '{exp_obj}'")
        success = False
        while not success:
            try:
                launch_experiments_jobs(task=dic_args["task"], exp_obj=exp_obj)
                success = True
            except:
                sleep(600)
